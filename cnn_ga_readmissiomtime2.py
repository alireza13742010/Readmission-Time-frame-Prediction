# -*- coding: utf-8 -*-
"""CNN_GA_readmissiomTime2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sICO3biixIdtWeDE38_vF7gEWIQTXPm6
"""

import numpy as np
from sklearn.metrics import classification_report
from sklearn import preprocessing
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
#from keras.utils import np_utils
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

data_set = pd.read_csv("/content/drive/MyDrive/diabetic_data.csv", na_values="?")

ds = data_set

import io
df2 = pd.read_csv('/content/drive/My Drive/diabetic_data.csv',na_values="?",low_memory=False)
target=df2['readmitted']
df2 = df2.fillna(df2.mode().iloc[0])

print(target.shape)

import io
import pandas as pd
df2 = pd.read_csv('/content/drive/My Drive/diabetic_data.csv')
target=df2['readmitted']
target_encoding=[]
for x in target:
  if x=='NO':
    target_encoding.append(np.array([0]))
  elif x=='<30':
    target_encoding.append(np.array([1/12]))
  elif x=='>30':
    target_encoding.append(np.array([0.54166666666]))
target_encoding[0]

from sklearn.preprocessing import OrdinalEncoder
import numpy as np
ordinal_encoder = OrdinalEncoder()
target1=np.array(target)
target1=target1.reshape(-1,1)
targets= ordinal_encoder.fit_transform(target1)

df2=df2.drop('readmitted', axis=1)
df2=df2.drop('encounter_id', axis=1)
df2=df2.drop('patient_nbr', axis=1)
df2=df2.drop('payer_code', axis=1)
df2.shape

!pip install category_encoders
import category_encoders as ce
column4=df2.columns
encoder4 = ce.TargetEncoder(cols=[x for x in column4])
target_encoding=np.array(targets)
target_encoder_all= encoder4.fit_transform(df2,target_encoding)
#target_encoder_all

target2=pd.get_dummies(target, prefix_sep='_', drop_first=False)
target2=np.array(target2)
print(target2.shape)
target3=target2.ravel()

X=target_encoder_all
y=target2
print(X.shape,y.shape)

ds.shape

#from imblearn.under_sampling import TomekLinks

#tl = TomekLinks(return_indices=True, ratio='majority')
#X_tl, y_tl, id_tl = tl.fit_sample(X, y)
#print(X_tl.shape, y_tl.shape)

ds.isnull().sum()

ds = data_set.copy()
#--------------------------------------------
ds.fillna(ds.mode().iloc[0], inplace=True)
#--------------------------------------------------
ds.drop_duplicates(subset ="patient_nbr", keep = False, inplace = True)
#--------------------------------------------------------
ds["A1Cresult"].replace({'None':'>8'}, inplace=True)
#---------------------------------------------
ds['max_glu_serum'].replace({'None':'Norm'}, inplace=True)
#-------------------------------------------------------
ds = ds[ds.gender != "Unknown/Invalid"]
#----------------------------------------------------------
ds["age"].replace({"[0-10)":1}, inplace=True )
ds["age"].replace({"[10-20)":1},inplace=True)
ds["age"].replace({"[20-30)":1},inplace=True)
ds["age"].replace({"[30-40)":1},inplace=True)
ds["age"].replace({"[40-50)":1},inplace=True)
ds["age"].replace({"[50-60)":1},inplace=True)
ds["age"].replace({"[60-70)":1.5},inplace=True)
ds["age"].replace({"[70-80)":1.5},inplace=True)
ds["age"].replace({"[80-90)":2},inplace=True)
ds["age"].replace({"[90-100)":2},inplace=True)
#-----------------------------------------------------
#ds["admission_source_id"].replace({8:2},inplace=True)
#ds["admission_source_id"].replace({22:2},inplace=True)
#ds["admission_source_id"].replace({10:2},inplace=True)
#ds["admission_source_id"].replace({11:2},inplace=True)
#ds["admission_source_id"].replace({14:2},inplace=True)
#ds["admission_source_id"].replace({25:2},inplace=True)
#ds["admission_source_id"].replace({13:2},inplace=True)
#ds["admission_source_id"].replace({5:3},inplace=True)
#ds["admission_source_id"].replace({3:3},inplace=True)
#ds["admission_source_id"].replace({20:3},inplace=True)
#ds["admission_source_id"].replace({9:3},inplace=True)
#ds["admission_source_id"].replace({17:4},inplace=True)
#ds["admission_source_id"].replace({4:4},inplace=True)
#ds["admission_source_id"].replace({6:4},inplace=True)
#ds["admission_source_id"].replace({2:4},inplace=True)
#-----------------------------------------------------
#ds["admission_type_id"].replace({6:4},inplace=True)
#ds["admission_type_id"].replace({5:4},inplace=True)
#ds["admission_type_id"].replace({8:5},inplace=True)
#ds["admission_type_id"].replace({7:5},inplace=True)
#ds["admission_type_id"].replace({4:5},inplace=True)
#-----------------------------------------------------
ds["A1Cresult"].replace({'None':'>8'}, inplace=True)
ds['max_glu_serum'].replace({'None':'Norm'}, inplace=True)
#------------------------------------------------------------
ds= ds[ds.gender != "Unknown/Invalid"]
#-------------------------------------------------------
ds= ds[ds.discharge_disposition_id != 11 ]
ds= ds[ds.discharge_disposition_id != 19]
ds= ds[ds.discharge_disposition_id != 20]
ds= ds[ds.discharge_disposition_id != 21]
#---------------------------------------------------
ds["medical_specialty"].fillna('Unknow', inplace=True)
#-------------------------------------------------------
ds["race"].fillna(ds['race'].mode()[0], inplace=True)

ds['gender'].value_counts()

print(f"Duplicated data = {len(ds)-len(ds.drop_duplicates())}")

for i, j in zip((list(ds.head(0))), np.arange(0, 50, 1)):
  print(f'the number of {i} column null values is: ', (ds[f'{i}'].isnull().sum()/ds.shape[0])*100,'%')

ds = ds.drop(["payer_code","weight","encounter_id"], axis=1)

ds.dropna(subset=["diag_1"], inplace = True)

ds["diag_3"].loc[ds["diag_2"].isnull()].isnull().sum()

ds["diag_2"].loc[ds["diag_2"].isnull()] = ds["diag_3"].loc[ds["diag_2"].isnull()].copy()

ds["diag_2"].loc[ds["diag_2"].isnull()] = ds["diag_1"].loc[ds["diag_2"].isnull()].copy()

ds["diag_3"].loc[ds["diag_3"].isnull()] = ds["diag_2"].loc[ds["diag_3"].isnull()].copy()

diag_cols = ['diag_1','diag_2','diag_3']
ds_copy = ds[diag_cols].copy()
for col in diag_cols:
    ds_copy[col] = ds_copy[col].str.replace('E','-')
    ds_copy[col] = ds_copy[col].str.replace('V','-')
    condition = ds_copy[col].str.contains('250')
    ds_copy.loc[condition,col] = '250'

ds_copy[diag_cols] = ds_copy[diag_cols].astype(float)
ds_copy.info
condition

for col in diag_cols:
    ds_copy['temp']=np.nan

    condition = (ds_copy[col]>=390) & (ds_copy[col]<=459) | (ds_copy[col]==785)
    ds_copy.loc[condition,'temp']='Circulatory'

    condition = (ds_copy[col]>=460) & (ds_copy[col]<=519) | (ds_copy[col]==786)
    ds_copy.loc[condition,'temp']='Respiratory'

    condition = (ds_copy[col]>=520) & (ds_copy[col]<=579) | (ds_copy[col]==787)
    ds_copy.loc[condition,'temp']='Digestive'

    condition = (ds_copy[col]>=800) & (ds_copy[col]<=999)
    ds_copy.loc[condition,'temp']='Injury'

    condition = (ds_copy[col]>=710) & (ds_copy[col]<=739)
    ds_copy.loc[condition,'temp']='Muscoloskeletal'

    condition = (ds_copy[col]>=580) & (ds_copy[col]<=629) | (ds_copy[col]==788)
    ds_copy.loc[condition,'temp']='Genitourinary'

    condition = (ds_copy[col]>=140) & (ds_copy[col]<=239) | (ds_copy[col]==780)
    ds_copy.loc[condition,'temp']='Neoplasms'

    condition = (ds_copy[col]>=240) & (ds_copy[col]<=279) | (ds_copy[col]==781)
    ds_copy.loc[condition,'temp']='Neoplasms'

    condition = (ds_copy[col]>=680) & (ds_copy[col]<=709) | (ds_copy[col]==782)
    ds_copy.loc[condition,'temp']='Neoplasms'

    condition = (ds_copy[col]>=790) & (ds_copy[col]<=799) | (ds_copy[col]==784)
    ds_copy.loc[condition,'temp']='Neoplasms'

    condition = (ds_copy[col]>=1) & (ds_copy[col]<=139)
    ds_copy.loc[condition,'temp']='Neoplasms'

    condition = (ds_copy[col]>=290) & (ds_copy[col]<=319)
    ds_copy.loc[condition,'temp']='Neoplasms'

    condition = (ds_copy[col]==250)
    ds_copy.loc[condition,'temp']='Diabetes'

    ds_copy['temp']=ds_copy['temp'].fillna('Others')
    condition = ds_copy['temp']=='0'
    ds_copy.loc[condition,'temp']=np.nan
    ds_copy[col]=ds_copy['temp']
    ds_copy.drop('temp',axis=1,inplace=True)
ds_copy

ds[diag_cols] = ds_copy.copy()
del ds_copy

ms_counts = ds["medical_specialty"].value_counts()
top = ms_counts[ms_counts>240]

ds.loc[~ds['medical_specialty'].isin(list(top.index)),'medical_specialty']='Other'

ds["max_glu_serum"].replace({'>200':1 ,'>300':1 ,'Norm':0 ,'None':-99}, inplace=True)

ds["A1Cresult"].replace({'>7':1 ,'>8':1 ,'Norm':0 ,'None':-99}, inplace=True)

ds['change'].replace('No', 0, inplace=True)
ds['gender'].replace('Male', 1, inplace=True)
ds['gender'].replace('Female', 0, inplace=True)
ds['diabetesMed'].replace('Yes', 1, inplace=True)
ds['diabetesMed'].replace('No', 0, inplace=True)

target = ds['readmitted']

len(target)

target3=ds['readmitted']
target_encoding=[]
for x in target3:
  if x=='NO':
    target_encoding.append(np.array([1,0,0]))
  elif x=='<30':
    target_encoding.append(np.array([0,1,0]))
  elif x=='>30':
    target_encoding.append(np.array([0,0,1]))
target_encoding3=np.array(target_encoding)
target_encoding3[1]

target5=pd.get_dummies(target3, prefix_sep='_', drop_first=False)
target5=np.array(target5)
print(target5.shape)

target3=ds['readmitted']
target_encoding=[]
for x in target3:
  if x=='NO':
    target_encoding.append(np.array([0]))
  elif x=='<30':
    target_encoding.append(np.array([1]))
  elif x=='>30':
    target_encoding.append(np.array([2]))
target_encoding4=np.array(target_encoding)
target_encoding4[1]

from sklearn.preprocessing import LabelEncoder

ds=ds.drop('readmitted', axis=1)
ds=ds.drop('patient_nbr', axis=1)
ds = ds.drop('medical_specialty',axis=1)

!pip install category_encoders
import category_encoders as ce
ds_cols1 = ds.columns
encoder = ce.TargetEncoder(cols= [x for x in ds_cols1])
target_encoding = np.array(target)
Tar_encoded_ds = encoder.fit_transform(ds, target_encoding4)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data_cat_conequtive=pd.DataFrame(target_encoding3)
data_cat_conequtive = data_cat_conequtive.apply(lambda col: le.fit_transform(col))

one_hot_r=pd.get_dummies(target3, prefix_sep='_', drop_first=False)
target5=np.array(one_hot_r)
X = Tar_encoded_ds
y = target_encoding3
print(X.shape,y.shape)

from sklearn.preprocessing import RobustScaler
transformer = RobustScaler().fit(X)
X = transformer.transform(X)

from sklearn.decomposition import PCA
pca = PCA()
pca.fit(X)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.9999) + 1
print(d)

pca = PCA(n_components=d)
X = pca.fit_transform(X)

X_TEST = X[45000:]
y_TEST = y[45000:]

i = np.where(y_TEST  == np.array([0,1,0]))
np.unique(i).shape

pd.DataFrame(y_TEST).value_counts()



X1 = X[0:45000]
y1 = y[0:45000]

X =X1
y = y1

from sklearn.preprocessing import OrdinalEncoder
import numpy as np
ordinal_encoder = OrdinalEncoder()
target1=np.array(target)
target1=target1.reshape(-1,1)
targets= ordinal_encoder.fit_transform(target1)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_sm, y_sm = smote.fit_resample(X, y)

from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler()
X_ros, y_ros = ros.fit_resample(X, y)
print(X_ros.shape, y_ros.shape)

from imblearn.under_sampling import TomekLinks

tl = TomekLinks()
X_tl, y_tl = tl.fit_resample(X, y)

from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler()
X_resampled, y_resampled = rus.fit_resample(X, y)
X_res_vis = pca.transform(X_resampled)

X_sm1, y_sm1 =pd.DataFrame(X_sm), pd.DataFrame (y_sm)
X_tl1, y_tl1  =pd.DataFrame(X_tl), pd.DataFrame (y_tl)
X_ros1, y_ros1  =pd.DataFrame(X_ros), pd.DataFrame (y_ros)

df5 = pd.concat([X_sm1,X_ros1])
print(df5.shape)
target5 = pd.concat([y_sm1,y_ros1])
print(target5.shape)

from sklearn.preprocessing import OneHotEncoder

target_combined=pd.get_dummies(target5, prefix_sep='_', drop_first=False)
target_combined = np.array(target_combined)
df6 = np.array(df5)
print(target_combined.shape)

from sklearn.model_selection import train_test_split
train_set, test_set,target_train,target_test = train_test_split(np.array(X1),y1, test_size=0.3,random_state=21)
#train_set, test_set,target_train,target_test = train_test_split(X_cc, y_cc, test_size=0.3,random_state=0)
print(target_train.shape,target_test.shape)
print(test_set.shape)
print(train_set.shape)

X_TEST = np.array(X_TEST)

train_set = train_set.reshape(train_set.shape[0],  train_set.shape[1],1)
test_set = X_TEST.reshape(X_TEST.shape[0],X_TEST.shape[1],1)
target_test = np.array(y_TEST)
print(target_test.shape)
print(test_set.shape)

int(train_set.shape[0]*0.8)

train_data=train_set[0:int(train_set.shape[0]*0.8)]
valid_data=train_set[int(train_set.shape[0]*0.8):]
train_target=target_train[0:int(train_set.shape[0]*0.8)]
valid_target=target_train[int(train_set.shape[0]*0.8):]

train_data= train_data.astype('float64')
valid_data = valid_data.astype('float64')
train_target = train_target.astype('float64')
valid_target = valid_target.astype('float64')

from tensorflow.keras import callbacks
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from tensorflow import nn
from tensorflow import keras
from tensorflow.python.keras.layers import Input, Dense, Dropout,Conv1D,MaxPooling1D,Activation,Flatten,AveragePooling1D,GlobalMaxPooling1D,ZeroPadding1D, GlobalAveragePooling1D
from tensorflow.python.keras.models import Sequential
from tensorflow import keras
from keras import layers

from tensorflow.keras.layers import BatchNormalization

from keras.backend import sigmoid
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))

from keras.backend import sigmoid
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))

from tensorflow.keras.saving import get_custom_objects
from keras.layers import Activation
get_custom_objects().update({'swish': Activation(swish)})

train_data= train_data.astype('float32')
valid_data = valid_data.astype('float32')
train_target = train_target.astype('float32')
valid_target = valid_target.astype('float32')

import tensorflow as tf
early_stopping_cb =  tf.keras.callbacks.EarlyStopping(patience=75,
restore_best_weights=True)
lr_scheduler1 = tf.keras.callbacks.ReduceLROnPlateau(factor=0.333, patience=20)

import tensorflow as tf
import os,datetime
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

kernel_init = keras.initializers.glorot_uniform()
bias_init = keras.initializers.Constant(value=0.2)

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from tensorflow.keras.saving import get_custom_objects
from keras.layers import Activation
get_custom_objects().update({'swish': Activation(swish)})

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from random import  randint
from random import choice
from random import uniform
from random import  randint
from random import choice
from sklearn.metrics import classification_report, confusion_matrix
from random import uniform
from keras import backend as K

from keras import backend as K
def f_measure(y_true, y_pred):
    """
    f_measure: Harmonic mean of specificity and sensitivity, shall be used to calculate score batch wise
    during training
    **for binary classification only**
    @param
    y_true: Tensor of actual labels
    y_pred: Tensor of predicted labels
    @returns
    f_measure score for a batch
    """
    def specificity(y_true, y_pred):
        """Compute the confusion matrix for a set of predictions.

        Parameters
        ----------
        y_pred   : predicted values for a batch if samples (must be binary: 0 or 1)
        y_true   : correct values for the set of samples used (must be binary: 0 or 1)

        Returns
        -------
        out : the specificity
        """
        neg_y_true = 1 - y_true
        neg_y_pred = 1 - y_pred
        fp = K.sum(neg_y_true * y_pred)
        tn = K.sum(neg_y_true * neg_y_pred)

        specificity = tn / (tn + fp + K.epsilon())
        return specificity

    def recall(y_true, y_pred):
        """Recall metric.

        Only computes a batch-wise average of recall.

        Computes the recall, a metric for multi-label classification of
        how many relevant items are selected.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    specificity = specificity(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((specificity * recall)/(specificity + recall + K.epsilon()))




def f1_score(y_true, y_pred):
    """Computes the F score.
     The F1 score is harmonic mean of precision and recall.
     it is computed as a batch-wise average.
     This is can be used for multi-label classification.
    """


    def precision(y_true, y_pred):
        """Precision metric.
         Only computes a batch-wise average of precision.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

    def recall(y_true, y_pred):
        """Recall metric.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    f1_score = 2 * (p * r) / (p + r + K.epsilon())
    return f1_score, p, r

!pip install tensorflow_addons

import tensorflow_addons as tfa

fl = tfa.losses.SigmoidFocalCrossEntropy(alpha=0.25, gamma = 3)

#time in hospital
class Net:
    def __init__(self):
        self.ep = randint(10000, 15000)               # epoch
        self.f1 = randint(30, 34)         # filter size 1
        self.f2 = randint(62, 66)            # filter size 2
        self.u1 = randint(250, 256)
        self.u2 = randint(125, 128)         # unit
        self.k1 = choice([(7),(9),(11),(13)])    # kernel size 1
        self.k2 = choice([(5),(7), (9), (11)])    # kernel size 2
        self.d1 = choice([0.15,0.25,0.35])         #  dropout 1
        self.d2 = choice([0.15,0.25,0.35])         #  dropout 2
        self.a1 = choice([('swish'),('tanh')])                      # activation 1
        self.a2 = choice([('swish'),('tanh')])                     # activation 2
        self.a3 = choice([('swish'),('tanh')])                     # activation 3
        self.a4 = 'softmax'                   # activation
        self.lf = choice(['categorical_crossentropy'])  # loss function
        self.op = choice([('adam'),('Nadam')])                  # optimization
        self.ac = 0                        # accuracy
        self.losss=np.inf

    def init_params(self):
        params = {'epochs': self.ep,
                  'filter1': self.f1,
                  'kernel1': self.k1,
                  'activation1': self.a1,
                  'filter2': self.f2,
                  'kernel2': self.k2,
                  'activation2': self.a2,
                  'pool_size': (2),
                  'dropout1': self.d1,
                  'unit1': self.u1,
                  'unit2':self.u2,
                  'activation3': self.a3,
                  'dropout2': self.d2,
                  'activation4': self.a4,
                  'loss': self.lf,
                  'optimizer': self.op,
                  'accuracy': self.ac,
                  'losss': self.losss}
        return params


def init_net(p):
    return [Net() for _ in range(p)]


def fitness(n, n_c, x, y, b, x_test, y_test):
    a=[]
    l=[]
    f=[]
    for cnt, i in enumerate(n):
        p = i.init_params()
        ep = p['epochs']
        f1 = p['filter1']
        f2 = p['filter2']
        k1 = p['kernel1']
        k2 = p['kernel2']
        d1 = p['dropout1']
        d2 = p['dropout2']
        ps = p['pool_size']
        u1 = p['unit1']
        u2 = p['unit2']
        a1 = p['activation1']
        a2 = p['activation2']
        a3 = p['activation3']
        a4 = p['activation4']
        lf = p['loss']
        op = p['optimizer']
        losss=p['losss']
        ac=p['accuracy']
        print('optimizer',op)
        print('activation1',a1)
        print('activation2',a2)
        print('activation3',a3)
        print('activation4',a4)
        print('dropout1',d1)
        print('dropout2',d2)
        print('kernel1',k1)
        print('kernel2',k2)
        print('filter1',f1)
        print('filter2',f2)
        print('number of neuron in unit1',u1)
        print('number of neuron in unit2',u2)
        print('Loss function is :',lf )



        try:                                # Parameter name    # Suggested value
            m = net_model(ep=ep,            # epoch number             12
                          f1=f1,            # filter size 1            32
                          f2=f2,            # filter size 2            64
                          k1=k1,            # kernel 1               (3)
                          k2=k2,            # kernel 2               (3)
                          a1=a1,            # activation 1           'relu'
                          a2=a2,            # activation 2           'relu'
                          a3=a3,            # activation 3           'relu'
                          a4=a4,            # activation 4           'softmax'
                          d1=d1,            # dropout 1                0.25
                          d2=d2,            # dropout 2                0.5
                          u1=u1,            # neuron number            256
                          u2=u2,            # neuron number            128
                          ps=ps,            # pool size               (2, 2)
                          op=op,            # optimizer               'adadelta'
                          lf=lf,            # loss function           'categorical crossentropy'
                          n_c=n_c,          # number of channel
                          # input shape
                          x=x,              # train data
                          y=y,              # train label
                          b=b,              # bias value
                          x_test=x_test,    # test data
                          y_test=y_test)    # test label

            # # Current best: 88%
            s = m.evaluate(x=x_test, y=y_test, verbose=2)


            Y_pred = m.predict(x_test)
            y_pred = np.argmax(Y_pred, axis=1)
            print('Confusion Matrix')
            print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))
            print('Classification Report')
            skplt.metrics.plot_confusion_matrix(np.argmax(y_test, axis=1), y_pred, normalize=True,cmap='Blues',figsize=[10,10])
            #m = tf.keras.metrics.Recall()
            #m.update_state(np.argmax(y_test, axis=1), y_pred)
            #print("Recall is ",m.result().numpy() )
            plt.show()
            i.ac = s[1]
            i.losss=s[0]

            print('Accuracy: {}'.format(i.ac * 100))
            print('loss: {}'.format(i.losss ))
            y_true0 = K.constant(y_test, dtype=tf.float32)
            y_pred0 = K.constant(Y_pred, dtype=tf.float32)
            print("***********************************")
            print('f1_score is:  ,percision is: and sensitivity is:', K.eval(f1_score(y_true=y_true0, y_pred=y_pred0)))
            if s[0]>1:
              f1=0.1*(s[1]*(0.15*(i.f1/34)+0.15*(i.f2/66)+0.15*(i.k1/13)+0.15*(i.k2/11)+0.1*(i.u1/256)+0.1*(i.u2/128)+0.2*(i.ep/2000)))+\
               0.9*(1/(s[0]+0.000001)*(0.15*(1-(i.f1/34))+0.15*(1-(i.f2/66))+0.15*(1-(i.k1/13))+0.15*(1-(i.k2/11))+0.1*(1-(i.u1/256))+0.1*(1-(i.u2/128))+0.2*(i.ep/2000)))
            else:
              f1=0.99*(s[1]*(0.15*(i.f1/34)+0.15*(i.f2/66)+0.15*(i.k1/13)+0.15*(i.k2/11)+0.1*(i.u1/256)+0.1*(i.u2/128)+0.2*(i.ep/2000)))+\
               0.01*(1/(s[0]+0.000001)*(0.15*(1-(i.f1/34))+0.15*(1-(i.f2/66))+0.15*(1-(i.k1/13))+0.15*(1-(i.k2/11))+0.1*(1-(i.u1/256))+0.1*(1-(i.u2/128))+0.2*(i.ep/2000)))

            f.append(f1)
            a.append(s[1])
            l.append(s[0])
#*(0.2*(i.f1/i.f1+i.f2)+0.2*(i.f2/i.f1+i.f2)+0.3*(i.k1/i.k1+i.k2)+0.1*(i.u1/i.u1+i.u2)+0.2*(i.ep/1.5*i.ep))
        except Exception as e:
            print(e)
    best_ac=np.argmax(a)
    best_l=np.argmin(l)
    best_f=np.argmax(f)
    if best_f:
      n=n[best_f]
    elif best_ac==best_l:
      n=n[best_ac]
    else:
      n=n[best_l]
    print(best_ac,best_l)
    return [n]


early_stopping_cb = keras.callbacks.EarlyStopping(patience=2500,
restore_best_weights=True)
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.readmissiom",
save_best_only=True)
def net_model(ep, f1, f2, k1, k2, a1, a2, a3, a4, d1, d2, u1, u2, ps, op, lf, n_c, x, y, b, x_test, y_test):
    model = Sequential()
    model.add(layer=Conv1D(filters=f1, kernel_size=k1, activation=a1,data_format='channels_last',padding='same'))
    model.add(layer=Conv1D(filters=f2, kernel_size=k2, activation=a2,data_format='channels_last',padding='same'))
    model.add(layer=Conv1D(filters=f2, kernel_size=1,strides=2, activation=a2,data_format='channels_last',padding='same'))
    model.add(layer=Dropout(rate=d1))
    model.add(layer=Flatten())
    model.add(layer=Dense(units=u1, activation=a3))
    model.add(layer= BatchNormalization())
    model.add(layer=Dropout(rate=d1))
    model.add(layer=Dense(units=u2, activation=a3))
    model.add(layer= BatchNormalization())
    model.add(layer=Dropout(rate=d2))
    model.add(layer=Dense(units=n_c, activation=a4))
    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)
    model.compile(optimizer=op, loss=lf)
    weights = {0:1, 1:30, 2:7}
    history = model.fit(x=x, y=y, batch_size=b, epochs=ep, verbose=2,validation_split=0.1,class_weight=weights,callbacks=[checkpoint_cb, early_stopping_cb])
    pd.DataFrame(history.history).plot(figsize=(8, 5))
    plt.grid(True)
    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
    plt.show()
    model.save("CNN_GA_Readmission.h5")
    return model
#model with skip

def selection(n):
    n = sorted(n, key=lambda j: j.ac, reverse=True)
    #n = sorted(n, key=lambda j: j.losss, reverse=True)
    n = n[:2]
    return n


def crossover(n):
    offspring = []
    p1 = choice(n)
    p2 = choice(n)
    c1 = Net()
    c2 = Net()
    c1.ep = int(p2.ep) + 2
    c2.ep = int(p1.ep) + 2
    offspring.append(c1)
    offspring.append(c2)
    n.extend(offspring)
    return n
def crossover1(n):
    offspring = []
    p3 = choice(n)
    p4 = choice(n)
    c3 = Net()
    c4 = Net()
    c3.u1 = int(p4.u1) + 10
    c4.u1 = int(p3.u1) + 10
    offspring.append(c3)
    offspring.append(c4)
    n.extend(offspring)
    return n

def crossover2(n):
    offspring = []
    p5 = choice(n)
    p6 = choice(n)
    c5 = Net()
    c6 = Net()
    c5.u1 = int(p5.k1) + 10
    c6.u1 = int(p6.k1) + 10
    offspring.append(c5)
    offspring.append(c6)
    n.extend(offspring)
    return n

def crossover3(n):
    offspring = []
    p7 = choice(n)
    p8 = choice(n)
    c7 = Net()
    c8 = Net()
    c7.u1 = int(p7.f1) + 10
    c8.u1 = int(p8.f1) + 10
    offspring.append(c7)
    offspring.append(c8)
    n.extend(offspring)
    return n


def mutate(n):
    for i in n:
        if uniform(0, 1) <= 0.10991:
            i.ep += randint(0, 5)
            i.u1 += randint(0, 10)
            i.f1 += randint(20, 60)
            i.f2 += randint(5, 10)
    return n

!pip install scikit-plot

import scikitplot as skplt
import matplotlib.pyplot as plt

with tf.device('/gpu:0'):
    P = 10 # Population
    G = 200 # Generation
    B = 2048# Batch size
    C = 3  # Class number
    T = 0.964  # Threshold
    N = init_net(p=P)  # Create population number networks
    train_set=train_set.reshape(train_set.shape[0],train_set.shape[1],1)
    test_set=test_set.reshape(test_set.shape[0],test_set.shape[1],1)
    train_set=train_set.astype('float32')
    test_set=test_set.astype('float32')
    target_train=target_train.astype('float32')
    target_test=target_test.astype('float32')
    X_tr, X_te, Y_tr, Y_te = train_set, test_set, target_train, target_test
    accuracy_list = []
    for g in range(G):
        print('Generation {}'.format(g + 1))
        N = fitness(n=N,
                    n_c=C,
                    x=X_tr,
                    y=Y_tr,
                    b=B,
                    x_test=X_te,

                    y_test=Y_te)
        N = selection(n=N)
        N = crossover(n=N)
        N = crossover1(n=N)
        N = crossover2(n=N)
        N = crossover3(n=N)
        N = mutate(n=N)

        for q in N:
            accuracy_list.append(q.ac * 100)
            if q.ac > T:
                print('Threshold satisfied')
                print(q.init_params())
                print('Best accuracy: {}%'.format(q.ac * 100))
                exit(code=0)

        print("The best accuracy so far {}%".format(max(accuracy_list)))

model = Sequential()
model.add(layer=Conv1D(filters=13, kernel_size=34, activation='tanh',data_format='channels_last',padding='same'))
model.add(layer=Conv1D(filters=5, kernel_size=66, activation='swish',data_format='channels_last',padding='same'))
model.add(layer=Conv1D(filters=5, kernel_size=1,strides=2, activation='swish',data_format='channels_last',padding='same'))
model.add(layer=Dropout(rate=0.25))
model.add(layer=Flatten())
model.add(layer=Dense(units=250, activation='tanh'))
model.add(layer= BatchNormalization())
model.add(layer=Dropout(rate=0.25))
model.add(layer=Dense(units=127, activation='tanh'))
model.add(layer= BatchNormalization())
model.add(layer=Dropout(rate=0.35))
model.add(layer=Dense(units=3, activation='softmax'))

checkpoint_cb = keras.callbacks.ModelCheckpoint("/content/CNN_GA_Readmission.h5")
model.compile(optimizer='Nadam', loss='categorical_crossentropy',metrics=['accuracy'])
history = model.fit(x=train_set , y=target_train, epochs=1000, verbose=2)